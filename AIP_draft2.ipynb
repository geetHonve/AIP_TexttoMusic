{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install audiolm_pytorch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VDXDSbxZY8sk",
        "outputId": "cdb343f7-2204-465e-8a01-7761e0f5c62a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting audiolm_pytorch\n",
            "  Downloading audiolm_pytorch-0.27.4-py3-none-any.whl (36 kB)\n",
            "Collecting accelerate\n",
            "  Downloading accelerate-0.18.0-py3-none-any.whl (215 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m215.3/215.3 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from audiolm_pytorch) (4.65.0)\n",
            "Collecting encodec\n",
            "  Downloading encodec-0.1.1.tar.gz (3.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.7/3.7 MB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.28.1-py3-none-any.whl (7.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m87.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting beartype\n",
            "  Downloading beartype-0.13.1-py3-none-any.whl (707 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m708.0/708.0 kB\u001b[0m \u001b[31m47.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting vector-quantize-pytorch>=1.2.2\n",
            "  Downloading vector_quantize_pytorch-1.2.2-py3-none-any.whl (10 kB)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.98-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m65.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fairseq\n",
            "  Downloading fairseq-0.12.2.tar.gz (9.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.6/9.6 MB\u001b[0m \u001b[31m86.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting ema-pytorch>=0.2.2\n",
            "  Downloading ema_pytorch-0.2.3-py3-none-any.whl (4.4 kB)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.9/dist-packages (from audiolm_pytorch) (1.2.2)\n",
            "Requirement already satisfied: torch>=1.12 in /usr/local/lib/python3.9/dist-packages (from audiolm_pytorch) (2.0.0+cu118)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.9/dist-packages (from audiolm_pytorch) (1.2.0)\n",
            "Collecting local-attention>=1.8.4\n",
            "  Downloading local_attention-1.8.5-py3-none-any.whl (8.1 kB)\n",
            "Collecting einops>=0.6.1\n",
            "  Downloading einops-0.6.1-py3-none-any.whl (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting lion-pytorch\n",
            "  Downloading lion_pytorch-0.0.7-py3-none-any.whl (4.3 kB)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.9/dist-packages (from audiolm_pytorch) (2.0.1+cu118)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch>=1.12->audiolm_pytorch) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torch>=1.12->audiolm_pytorch) (2.0.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch>=1.12->audiolm_pytorch) (3.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch>=1.12->audiolm_pytorch) (4.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from torch>=1.12->audiolm_pytorch) (3.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch>=1.12->audiolm_pytorch) (1.11.1)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.12->audiolm_pytorch) (16.0.1)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.12->audiolm_pytorch) (3.25.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.9/dist-packages (from accelerate->audiolm_pytorch) (6.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from accelerate->audiolm_pytorch) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from accelerate->audiolm_pytorch) (23.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.9/dist-packages (from accelerate->audiolm_pytorch) (5.9.5)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.9/dist-packages (from fairseq->audiolm_pytorch) (2022.10.31)\n",
            "Collecting bitarray\n",
            "  Downloading bitarray-2.7.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (269 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m269.6/269.6 kB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting omegaconf<2.1\n",
            "  Downloading omegaconf-2.0.6-py3-none-any.whl (36 kB)\n",
            "Requirement already satisfied: cffi in /usr/local/lib/python3.9/dist-packages (from fairseq->audiolm_pytorch) (1.15.1)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.9/dist-packages (from fairseq->audiolm_pytorch) (0.29.34)\n",
            "Collecting hydra-core<1.1,>=1.0.7\n",
            "  Downloading hydra_core-1.0.7-py3-none-any.whl (123 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.8/123.8 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sacrebleu>=1.4.12\n",
            "  Downloading sacrebleu-2.3.1-py3-none-any.whl (118 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.9/118.9 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.9/dist-packages (from scikit-learn->audiolm_pytorch) (1.10.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn->audiolm_pytorch) (3.1.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m40.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers->audiolm_pytorch) (2.27.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers->audiolm_pytorch) (2023.4.0)\n",
            "Collecting antlr4-python3-runtime==4.8\n",
            "  Downloading antlr4-python3-runtime-4.8.tar.gz (112 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.4/112.4 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting colorama\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.9/dist-packages (from sacrebleu>=1.4.12->fairseq->audiolm_pytorch) (0.8.10)\n",
            "Collecting portalocker\n",
            "  Downloading portalocker-2.7.0-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.9/dist-packages (from sacrebleu>=1.4.12->fairseq->audiolm_pytorch) (4.9.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.9/dist-packages (from cffi->fairseq->audiolm_pytorch) (2.21)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch>=1.12->audiolm_pytorch) (2.1.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers->audiolm_pytorch) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers->audiolm_pytorch) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers->audiolm_pytorch) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers->audiolm_pytorch) (3.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch>=1.12->audiolm_pytorch) (1.3.0)\n",
            "Building wheels for collected packages: encodec, fairseq, antlr4-python3-runtime\n",
            "  Building wheel for encodec (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for encodec: filename=encodec-0.1.1-py3-none-any.whl size=45775 sha256=27473926413734b5af7f291cfe9671b134780676b44365718f476094c104cd57\n",
            "  Stored in directory: /root/.cache/pip/wheels/1d/9d/20/489d6aafffb505e18fcfcfbe722562f91c26af0a8a6da7d00b\n",
            "  Building wheel for fairseq (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fairseq: filename=fairseq-0.12.2-cp39-cp39-linux_x86_64.whl size=11180053 sha256=ac374bf2027a8a537bb37c1e20a3f18d58e17a72062d45423db865a11ff4f383\n",
            "  Stored in directory: /root/.cache/pip/wheels/59/35/87/2baf2e4ad37c83fd698c486b3d39f0e7022226fa52ab469c31\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.8-py3-none-any.whl size=141229 sha256=bfefaea8ec5f3a13cb9c07339e5801b3785ce39c7d7c757a1b79f35e0daef4d7\n",
            "  Stored in directory: /root/.cache/pip/wheels/42/3c/ae/14db087e6018de74810afe32eb6ac890ef9c68ba19b00db97a\n",
            "Successfully built encodec fairseq antlr4-python3-runtime\n",
            "Installing collected packages: tokenizers, sentencepiece, bitarray, antlr4-python3-runtime, portalocker, omegaconf, einops, colorama, beartype, sacrebleu, hydra-core, huggingface-hub, transformers, vector-quantize-pytorch, local-attention, lion-pytorch, fairseq, encodec, ema-pytorch, accelerate, audiolm_pytorch\n",
            "Successfully installed accelerate-0.18.0 antlr4-python3-runtime-4.8 audiolm_pytorch-0.27.4 beartype-0.13.1 bitarray-2.7.3 colorama-0.4.6 einops-0.6.1 ema-pytorch-0.2.3 encodec-0.1.1 fairseq-0.12.2 huggingface-hub-0.14.1 hydra-core-1.0.7 lion-pytorch-0.0.7 local-attention-1.8.5 omegaconf-2.0.6 portalocker-2.7.0 sacrebleu-2.3.1 sentencepiece-0.1.98 tokenizers-0.13.3 transformers-4.28.1 vector-quantize-pytorch-1.2.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install x_clip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "noWD3mKjZY1z",
        "outputId": "77a4d9d2-2279-434f-b10c-209713739c23"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting x_clip\n",
            "  Downloading x_clip-0.12.1-py3-none-any.whl (1.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ftfy\n",
            "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torchvision in /usr/local/lib/python3.9/dist-packages (from x_clip) (0.15.1+cu118)\n",
            "Requirement already satisfied: einops>=0.3 in /usr/local/lib/python3.9/dist-packages (from x_clip) (0.6.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.9/dist-packages (from x_clip) (2022.10.31)\n",
            "Requirement already satisfied: torch>=1.6 in /usr/local/lib/python3.9/dist-packages (from x_clip) (2.0.0+cu118)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch>=1.6->x_clip) (1.11.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch>=1.6->x_clip) (4.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from torch>=1.6->x_clip) (3.11.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch>=1.6->x_clip) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torch>=1.6->x_clip) (2.0.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch>=1.6->x_clip) (3.1)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.6->x_clip) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.6->x_clip) (16.0.1)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.9/dist-packages (from ftfy->x_clip) (0.2.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from torchvision->x_clip) (2.27.1)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.9/dist-packages (from torchvision->x_clip) (8.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from torchvision->x_clip) (1.22.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch>=1.6->x_clip) (2.1.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision->x_clip) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision->x_clip) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision->x_clip) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision->x_clip) (1.26.15)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch>=1.6->x_clip) (1.3.0)\n",
            "Installing collected packages: ftfy, x_clip\n",
            "Successfully installed ftfy-6.1.1 x_clip-0.12.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install einops"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xUjNfkRtZj9P",
        "outputId": "12ee57e5-2317-4d88-8e4a-1507a9ab62a3"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.9/dist-packages (0.6.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install beartype"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-oNY7hnoZpyA",
        "outputId": "5dc568f8-4fe7-4c72-e5a4-76fd9547c45d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: beartype in /usr/local/lib/python3.9/dist-packages (0.13.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "YWlykoGuXiCF"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "from functools import wraps, partial\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn, einsum\n",
        "\n",
        "from torchaudio.transforms import Spectrogram, TimeStretch, FrequencyMasking, TimeMasking\n",
        "\n",
        "from audiolm_pytorch import AudioLM\n",
        "from audiolm_pytorch.utils import AudioConditionerBase\n",
        "\n",
        "from x_clip.tokenizer import tokenizer\n",
        "from vector_quantize_pytorch import ResidualVQ\n",
        "\n",
        "from einops import rearrange, repeat, reduce, pack, unpack\n",
        "from einops.layers.torch import Rearrange\n",
        "\n",
        "from beartype.typing import List, Optional, Tuple\n",
        "from beartype import beartype\n",
        "\n",
        "# functions\n",
        "\n",
        "def exists(val):\n",
        "    return val is not None\n",
        "\n",
        "def first(it):\n",
        "    return it[0]\n",
        "\n",
        "def default(val, d):\n",
        "    return val if exists(val) else d\n",
        "\n",
        "def round_down_nearest_multiple(n, divisor):\n",
        "    return n // divisor * divisor\n",
        "\n",
        "def Sequential(*modules):\n",
        "    return nn.Sequential(*filter(exists, modules))\n",
        "\n",
        "# decorators\n",
        "\n",
        "def once(fn):\n",
        "    called = False\n",
        "    @wraps(fn)\n",
        "    def inner(x):\n",
        "        nonlocal called\n",
        "        if called:\n",
        "            return\n",
        "        called = True\n",
        "        return fn(x)\n",
        "    return inner\n",
        "\n",
        "print_once = once(print)\n",
        "\n",
        "# tensor functions\n",
        "\n",
        "def log(t, eps = 1e-20):\n",
        "    return torch.log(t.clamp(min = eps))\n",
        "\n",
        "def l2norm(t):\n",
        "    return F.normalize(t, p = 2, dim = -1)\n",
        "\n",
        "def matrix_diag(t):\n",
        "    device = t.device\n",
        "    i, j = t.shape[-2:]\n",
        "    num_diag_el = min(i, j)\n",
        "    i_range = torch.arange(i, device = device)\n",
        "    j_range = torch.arange(j, device = device)\n",
        "    diag_mask = rearrange(i_range, 'i -> i 1') == rearrange(j_range, 'j -> 1 j')\n",
        "    diag_el = t.masked_select(diag_mask)\n",
        "    return rearrange(diag_el, '(b d) -> b d', d = num_diag_el)\n",
        "\n",
        "# 2d sinusoidal positional embedding\n",
        "# simple vit paper shows it is good enough compared to learned\n",
        "\n",
        "def posemb_sincos_2d(patches, temperature = 10000, dtype = torch.float32):\n",
        "    _, h, w, dim, device, dtype = *patches.shape, patches.device, patches.dtype\n",
        "\n",
        "    y, x = torch.meshgrid(torch.arange(h, device = device), torch.arange(w, device = device), indexing = 'ij')\n",
        "    assert (dim % 4) == 0, 'feature dimension must be multiple of 4 for sincos emb'\n",
        "\n",
        "    omega = torch.arange(dim // 4, device = device) / (dim // 4 - 1)\n",
        "    omega = 1. / (temperature ** omega)\n",
        "\n",
        "    y = y.flatten()[:, None] * omega[None, :]\n",
        "    x = x.flatten()[:, None] * omega[None, :] \n",
        "\n",
        "    pe = torch.cat((x.sin(), x.cos(), y.sin(), y.cos()), dim = 1)\n",
        "    pe = pe.type(dtype)\n",
        "\n",
        "    return rearrange(pe, '(h w) d -> h w d', h = h, w = w)\n",
        "\n",
        "# biasless layernorm\n",
        "\n",
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self, dim, scale = True):\n",
        "        super().__init__()\n",
        "        self.learned_gamma = nn.Parameter(torch.ones(dim)) if scale else None\n",
        "\n",
        "        self.register_buffer('gamma', torch.ones(dim), persistent = False)\n",
        "        self.register_buffer('beta', torch.zeros(dim), persistent = False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return F.layer_norm(x, x.shape[-1:], default(self.learned_gamma, self.gamma), self.beta)\n",
        "\n",
        "# feedforward\n",
        "\n",
        "class GEGLU(nn.Module):\n",
        "    def forward(self, x):\n",
        "        x, gate = x.chunk(2, dim = -1)\n",
        "        return F.gelu(gate) * x\n",
        "\n",
        "def FeedForward(dim, mult = 4, dropout = 0.):\n",
        "    dim_hidden = int(dim * mult * 2 / 3)\n",
        "\n",
        "    return nn.Sequential(\n",
        "        LayerNorm(dim),\n",
        "        nn.Linear(dim, dim_hidden * 2, bias = False),\n",
        "        GEGLU(),\n",
        "        nn.Dropout(dropout),\n",
        "        nn.Linear(dim_hidden, dim, bias = False)\n",
        "    )\n",
        "\n",
        "# attention\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim,\n",
        "        causal = False,\n",
        "        dim_head = 64,\n",
        "        heads = 8,\n",
        "        dropout = 0.,\n",
        "        scale = 8\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.heads = heads\n",
        "        self.scale = scale\n",
        "        self.causal = causal\n",
        "        inner_dim = dim_head * heads\n",
        "\n",
        "        self.norm = LayerNorm(dim)\n",
        "\n",
        "        self.attn_dropout = nn.Dropout(dropout)\n",
        "\n",
        "        self.to_q = nn.Linear(dim, inner_dim, bias = False)\n",
        "        self.to_kv = nn.Linear(dim, inner_dim * 2, bias = False)\n",
        "\n",
        "        self.q_scale = nn.Parameter(torch.ones(dim_head))\n",
        "        self.k_scale = nn.Parameter(torch.ones(dim_head))\n",
        "\n",
        "        self.to_out = nn.Sequential(\n",
        "            nn.Linear(inner_dim, dim, bias = False),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        x,\n",
        "        rel_pos_bias = None,\n",
        "        mask = None\n",
        "    ):\n",
        "        b, n, _, device = *x.shape, x.device\n",
        "\n",
        "        # prenorm\n",
        "\n",
        "        x = self.norm(x)\n",
        "\n",
        "        # project for queries, keys, values\n",
        "\n",
        "        q, k, v = self.to_q(x), *self.to_kv(x).chunk(2, dim = -1)\n",
        "\n",
        "        # split for multi-headed attention\n",
        "\n",
        "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = self.heads), (q, k, v))\n",
        "\n",
        "        # qk rmsnorm, technique circulating within brain used to stabilize a 22B parameter vision model training\n",
        "\n",
        "        q, k = map(l2norm, (q, k))\n",
        "        q = q * self.q_scale\n",
        "        k = k * self.k_scale\n",
        "\n",
        "        # similarities\n",
        "\n",
        "        sim = einsum('b h i d, b h j d -> b h i j', q, k) * self.scale\n",
        "\n",
        "        if exists(rel_pos_bias):\n",
        "            sim = sim + rel_pos_bias\n",
        "\n",
        "        if exists(mask):\n",
        "            mask = rearrange(mask, 'b j -> b 1 1 j')\n",
        "            sim = sim.masked_fill(~mask, -torch.finfo(sim.dtype).max)\n",
        "\n",
        "        if self.causal:\n",
        "            i, j = sim.shape[-2:]\n",
        "            causal_mask = torch.ones((i, j), dtype = torch.bool, device = x.device).triu(j - i + 1)\n",
        "            sim = sim.masked_fill(causal_mask, -torch.finfo(sim.dtype).max)\n",
        "\n",
        "        # attention\n",
        "\n",
        "        attn = sim.softmax(dim = -1)\n",
        "        attn = self.attn_dropout(attn)\n",
        "\n",
        "        # aggregate\n",
        "\n",
        "        out = einsum('b h i j, b h j d -> b h i d', attn, v)\n",
        "\n",
        "        # merge heads\n",
        "\n",
        "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
        "        return self.to_out(out)\n",
        "\n",
        "# transformer\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim,\n",
        "        depth,\n",
        "        dim_head = 64,\n",
        "        heads = 8,\n",
        "        attn_dropout = 0.,\n",
        "        ff_mult = 4,\n",
        "        ff_dropout = 0.\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([])\n",
        "        for _ in range(depth):\n",
        "            self.layers.append(nn.ModuleList([\n",
        "                Attention(dim = dim, dim_head = dim_head, heads = heads, dropout = attn_dropout),\n",
        "                FeedForward(dim = dim, mult = ff_mult, dropout = ff_dropout),\n",
        "            ]))\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        x,\n",
        "        rel_pos_bias = None,\n",
        "        mask = None,\n",
        "        return_all_layers = False\n",
        "    ):\n",
        "        layers = []\n",
        "\n",
        "        for attn, ff in self.layers:\n",
        "            x = attn(x, rel_pos_bias = rel_pos_bias, mask = mask) + x\n",
        "            x = ff(x) + x\n",
        "            layers.append(x)\n",
        "\n",
        "        if not return_all_layers:\n",
        "            return x\n",
        "\n",
        "        return x, torch.stack(layers[:-1])\n",
        "\n",
        "# contrastive losses\n",
        "\n",
        "class SoftmaxContrastiveLearning(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        *,\n",
        "        layers = 1,\n",
        "        decoupled_contrastive_learning = False,\n",
        "        init_temp = 10\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.temperatures = nn.Parameter(torch.ones(layers, 1, 1) * math.log(init_temp))\n",
        "        self.decoupled_contrastive_learning = decoupled_contrastive_learning\n",
        "\n",
        "    @property\n",
        "    def device(self):\n",
        "        return next(self.parameters()).device\n",
        "\n",
        "    def forward(self, sims):\n",
        "        batch = sims.shape[-1]\n",
        "\n",
        "        if sims.ndim == 2:\n",
        "            sims = rearrange(sims, 'i j -> 1 i j')\n",
        "\n",
        "        sims = sims * self.temperatures.exp()\n",
        "\n",
        "        cosine_sims_exp = sims.exp()\n",
        "\n",
        "        numerator = matrix_diag(cosine_sims_exp)\n",
        "\n",
        "        if self.decoupled_contrastive_learning:\n",
        "            eye = torch.eye(batch, device = self.device, dtype = torch.bool)\n",
        "            cosine_sims_exp = cosine_sims_exp.masked_fill(eye, 0.)\n",
        "\n",
        "        denominator_i = reduce(cosine_sims_exp, 'l i j -> l i', 'sum')\n",
        "        denominator_j = reduce(cosine_sims_exp, 'l i j -> l j', 'sum')\n",
        "\n",
        "        contrastive_loss = -log(numerator) + 0.5 * (log(denominator_i) + log(denominator_j))\n",
        "\n",
        "        contrastive_loss = reduce(contrastive_loss, 'l n -> l', 'mean')\n",
        "        return contrastive_loss.sum()\n",
        "\n",
        "class SigmoidContrastiveLearning(nn.Module):\n",
        "    \"\"\" https://arxiv.org/abs/2303.15343 \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        *,\n",
        "        layers = 1,\n",
        "        init_temp = 10,\n",
        "        init_bias = -10\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.temperatures = nn.Parameter(torch.ones(layers, 1, 1) * math.log(init_temp))\n",
        "        self.bias = nn.Parameter(torch.ones(layers, 1, 1) * init_bias)\n",
        "\n",
        "    @property\n",
        "    def device(self):\n",
        "        return next(self.parameters()).device\n",
        "\n",
        "    def forward(self, sims):\n",
        "        if sims.ndim == 2:\n",
        "            sims = rearrange(sims, 'i j -> 1 i j')\n",
        "\n",
        "        n = sims.shape[-1]\n",
        "        sims = sims * self.temperatures.exp() + self.bias\n",
        "        labels = 2 * rearrange(torch.eye(n), 'i j -> 1 i j') - torch.ones_like(sims)\n",
        "\n",
        "        return -F.logsigmoid(labels * sims).sum() / n\n",
        "\n",
        "# Audio Spectrogram Transformer - https://arxiv.org/abs/2104.01778\n",
        "\n",
        "def pair(t):\n",
        "    return (t, t) if not isinstance(t, tuple) else t\n",
        "\n",
        "class AudioSpectrogramTransformer(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim,\n",
        "        depth,\n",
        "        patch_size = 16,\n",
        "        dim_head = 64,\n",
        "        heads = 8,\n",
        "        attn_dropout = 0.,\n",
        "        ff_mult = 4,\n",
        "        ff_dropout = 0.,\n",
        "        accept_spec = False,\n",
        "        accept_spec_time_first = True,\n",
        "        spec_n_fft = 128,\n",
        "        spec_power = 2,\n",
        "        spec_win_length = 24,\n",
        "        spec_hop_length = None,\n",
        "        spec_pad = 0,\n",
        "        spec_center = True,\n",
        "        spec_pad_mode = 'reflect',\n",
        "        spec_aug_stretch_factor = 0.8,\n",
        "        spec_aug_freq_mask = 80,\n",
        "        spec_aug_time_mask = 80,\n",
        "        patch_dropout_prob = 0.25\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.depth = depth\n",
        "\n",
        "        self.patch_size = pair(patch_size)\n",
        "        patch_input_dim = self.patch_size[0] * self.patch_size[1]\n",
        "\n",
        "        self.to_patch_tokens = Sequential(\n",
        "            Rearrange('b (h p1) (w p2) -> b h w (p1 p2)', p1 = self.patch_size[0], p2 = self.patch_size[1]),\n",
        "            nn.LayerNorm(patch_input_dim),\n",
        "            nn.Linear(patch_input_dim, dim),\n",
        "            nn.LayerNorm(dim)\n",
        "        )\n",
        "\n",
        "        self.accept_spec = accept_spec\n",
        "        self.accept_spec_time_first = accept_spec_time_first\n",
        "\n",
        "        self.spec = Spectrogram(\n",
        "            n_fft = spec_n_fft,\n",
        "            power = spec_power,\n",
        "            win_length = spec_win_length,\n",
        "            hop_length = spec_hop_length,\n",
        "            pad = spec_pad,\n",
        "            center = spec_center,\n",
        "            pad_mode = spec_pad_mode\n",
        "        )\n",
        "\n",
        "        # SpecAugment - seems to be widely used in audio field https://arxiv.org/abs/1904.08779\n",
        "\n",
        "        self.aug = torch.nn.Sequential(\n",
        "            TimeStretch(spec_aug_stretch_factor, fixed_rate = True),\n",
        "            FrequencyMasking(freq_mask_param = spec_aug_freq_mask),\n",
        "            TimeMasking(time_mask_param = spec_aug_time_mask),\n",
        "        )\n",
        "\n",
        "        self.transformer = Transformer(\n",
        "            dim = dim,\n",
        "            depth = depth,\n",
        "            dim_head = dim_head,\n",
        "            heads = heads,\n",
        "            attn_dropout = attn_dropout,\n",
        "            ff_mult = ff_mult,\n",
        "            ff_dropout = ff_dropout\n",
        "        )\n",
        "\n",
        "        self.norm = LayerNorm(dim)\n",
        "\n",
        "        # patch dropout\n",
        "\n",
        "        self.patch_dropout_prob = patch_dropout_prob\n",
        "\n",
        "        # 2d dynamic positional bias\n",
        "\n",
        "        mlp_hidden_dim = dim // 4\n",
        "\n",
        "        self.dynamic_pos_bias_mlp = nn.Sequential(\n",
        "            nn.Linear(2, mlp_hidden_dim),\n",
        "            nn.SiLU(),\n",
        "            nn.Linear(mlp_hidden_dim, mlp_hidden_dim),\n",
        "            nn.SiLU(),\n",
        "            nn.Linear(mlp_hidden_dim, heads),\n",
        "            Rearrange('... i j h -> ... h i j')\n",
        "        )\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        x,\n",
        "        force_no_patch_dropout = False,\n",
        "        return_all_layers = False\n",
        "    ):\n",
        "        batch, device = x.shape[0], x.device\n",
        "        assert (self.accept_spec and x.ndim == 3) or (not self.accept_spec and x.ndim == 2)\n",
        "\n",
        "        if self.accept_spec and self.accept_spec_time_first:\n",
        "            x = rearrange(x, 'b t f -> b f t')\n",
        "\n",
        "        if not self.accept_spec:\n",
        "            x = self.spec(x)\n",
        "\n",
        "        if self.training:\n",
        "            x = self.aug(x)\n",
        "\n",
        "        # automatically crop if audio does not yield a 2d spectrogram that is divisible by patch sizes\n",
        "\n",
        "        height, width = x.shape[-2:]\n",
        "        patch_height, patch_width = self.patch_size\n",
        "\n",
        "        rounded_height, rounded_width = map(lambda args: round_down_nearest_multiple(*args), ((height, patch_height), (width, patch_width)))\n",
        "\n",
        "        if (height, width) != (rounded_height, rounded_width): # just keep printing to be annoying until it is fixed\n",
        "            print_once(f'spectrogram yielded shape of {(height, width)}, but had to be cropped to {(rounded_height, rounded_width)} to be patchified for transformer')\n",
        "\n",
        "        x = x[..., :rounded_height, :rounded_width]\n",
        "\n",
        "        # to patches\n",
        "\n",
        "        x = self.to_patch_tokens(x)\n",
        "\n",
        "        # get number of patches along height and width\n",
        "\n",
        "        _, num_patch_height, num_patch_width, _ = x.shape\n",
        "\n",
        "        # get 2d relative positions\n",
        "\n",
        "        grid = torch.stack(torch.meshgrid(\n",
        "            torch.arange(num_patch_height, device = device),\n",
        "            torch.arange(num_patch_width, device = device)\n",
        "        , indexing = 'ij'), dim = -1)\n",
        "\n",
        "        grid = rearrange(grid, '... c -> (...) c')\n",
        "\n",
        "        # 2d sinusoidal positional embedding\n",
        "\n",
        "        x = x + posemb_sincos_2d(x)\n",
        "\n",
        "        x = rearrange(x, 'b ... c -> b (...) c')\n",
        "\n",
        "        # patch dropout\n",
        "\n",
        "        if self.training and self.patch_dropout_prob > 0. and not force_no_patch_dropout:\n",
        "            n, device = x.shape[1], x.device\n",
        "\n",
        "            batch_indices = torch.arange(batch, device = device)\n",
        "            batch_indices = rearrange(batch_indices, '... -> ... 1')\n",
        "            num_patches_keep = max(1, int(n * (1 - self.patch_dropout_prob)))\n",
        "            patch_indices_keep = torch.randn(batch, n, device = device).topk(num_patches_keep, dim = -1).indices\n",
        "\n",
        "            x = x[batch_indices, patch_indices_keep]\n",
        "\n",
        "            grid = repeat(grid, '... -> b ...', b = batch)\n",
        "            grid = grid[batch_indices, patch_indices_keep]\n",
        "\n",
        "        # 2d relative positional bias\n",
        "\n",
        "        rel_dist = rearrange(grid, '... i c -> ... i 1 c') - rearrange(grid, '... j c -> ... 1 j c')\n",
        "        rel_pos_bias = self.dynamic_pos_bias_mlp(rel_dist.float())\n",
        "\n",
        "        # attention, what else\n",
        "\n",
        "        x, all_layers = self.transformer(x, rel_pos_bias = rel_pos_bias, return_all_layers = True)\n",
        "\n",
        "        # final global average and norm (most recent papers show this is superior to CLS token)\n",
        "\n",
        "        x = reduce(x, 'b n d -> b d', 'mean')\n",
        "\n",
        "        out = self.norm(x)\n",
        "\n",
        "        if not return_all_layers:\n",
        "            return out\n",
        "\n",
        "        return out, all_layers\n",
        "\n",
        "# text transformer\n",
        "\n",
        "class TextTransformer(nn.Module):\n",
        "    @beartype\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim,\n",
        "        depth,\n",
        "        num_tokens = tokenizer.vocab_size,\n",
        "        max_seq_len = 256,\n",
        "        dim_head = 64,\n",
        "        heads = 8,\n",
        "        attn_dropout = 0.,\n",
        "        ff_dropout = 0.,\n",
        "        ff_mult = 4,\n",
        "        pad_id = 0\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "\n",
        "        self.token_emb = nn.Embedding(num_tokens, dim)\n",
        "        self.pos_emb = nn.Embedding(max_seq_len, dim)\n",
        "\n",
        "        self.depth = depth\n",
        "        self.max_seq_len = max_seq_len\n",
        "\n",
        "        self.cls_token = nn.Parameter(torch.randn(dim))\n",
        "\n",
        "        self.transformer = Transformer(\n",
        "            dim = dim,\n",
        "            depth = depth,\n",
        "            dim_head = dim_head,\n",
        "            heads = heads,\n",
        "            attn_dropout = attn_dropout,\n",
        "            ff_dropout = ff_dropout,\n",
        "            ff_mult = ff_mult\n",
        "        )\n",
        "\n",
        "        self.pad_id = pad_id\n",
        "        self.norm = LayerNorm(dim)\n",
        "\n",
        "    @property\n",
        "    def device(self):\n",
        "        return next(self.parameters()).device\n",
        "\n",
        "    @beartype\n",
        "    def forward(\n",
        "        self,\n",
        "        x = None,\n",
        "        raw_texts: Optional[List[str]] = None,\n",
        "        mask = None,\n",
        "        return_all_layers = False\n",
        "    ):\n",
        "        assert exists(x) ^ exists(raw_texts)\n",
        "\n",
        "        if exists(raw_texts):\n",
        "            x = tokenizer.tokenize(raw_texts).to(self.device)\n",
        "\n",
        "        if not exists(mask):\n",
        "            mask = x != self.pad_id\n",
        "\n",
        "        b, n, device = *x.shape, x.device\n",
        "\n",
        "        # token embedding + positional embedding\n",
        "\n",
        "        x = self.token_emb(x)\n",
        "\n",
        "        assert n <= self.max_seq_len, f'text sequence length {n} must be less than {self.max_seq_len}'\n",
        "\n",
        "        x = x + self.pos_emb(torch.arange(n, device = device))\n",
        "\n",
        "        # cls tokens, as in bert\n",
        "\n",
        "        cls_tokens = repeat(self.cls_token, 'd -> b d', b = b)\n",
        "        x, ps = pack([cls_tokens, x], 'b * d')\n",
        "\n",
        "        # account for attending to cls token with self attention mask\n",
        "\n",
        "        mask = F.pad(mask, (1, 0), value = True)\n",
        "\n",
        "        # attention\n",
        "\n",
        "        x, all_layers = self.transformer(x, mask = mask, return_all_layers = True)\n",
        "\n",
        "        # unpack the cls tokens\n",
        "\n",
        "        cls_tokens, _ = unpack(x, ps, 'b * d')\n",
        "\n",
        "        out = self.norm(cls_tokens)\n",
        "\n",
        "        if not return_all_layers:\n",
        "            return out\n",
        "\n",
        "        return out, all_layers\n",
        "\n",
        "# hierarchical cl loss\n",
        "\n",
        "def interspersed_indices(layers, total_layers):\n",
        "    assert total_layers >= layers\n",
        "    step = total_layers / layers\n",
        "    return (torch.arange(0, layers) * step).floor().long()\n",
        "\n",
        "class MultiLayerContrastiveLoss(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        *,\n",
        "        audio_dim,\n",
        "        text_dim,\n",
        "        dim_latent,\n",
        "        layers,\n",
        "        decoupled_contrastive_learning = False,\n",
        "        sigmoid_contrastive_loss = False\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.layers = layers\n",
        "\n",
        "        self.audio_norm = LayerNorm(audio_dim, scale = False)\n",
        "        self.audio_gamma = nn.Parameter(torch.ones(layers, 1, audio_dim))\n",
        "        self.audio_latent_weight = nn.Parameter(torch.randn(layers, audio_dim, dim_latent))\n",
        "        self.audio_latent_bias = nn.Parameter(torch.randn(layers, 1, dim_latent))\n",
        "\n",
        "        self.text_norm = LayerNorm(text_dim, scale = False)\n",
        "        self.text_gamma = nn.Parameter(torch.ones(layers, 1, text_dim))\n",
        "        self.text_latent_weight = nn.Parameter(torch.randn(layers, text_dim, dim_latent))\n",
        "        self.text_latent_bias = nn.Parameter(torch.randn(layers, 1, dim_latent))\n",
        "\n",
        "        klass = SigmoidContrastiveLearning if sigmoid_contrastive_loss else partial(SoftmaxContrastiveLearning, decoupled_contrastive_learning = decoupled_contrastive_learning)\n",
        "        self.contrast = klass(layers = layers)\n",
        "\n",
        "    def forward(self, *, audio_layers, text_layers):\n",
        "        device, batch = audio_layers.device, audio_layers.shape[1]\n",
        "\n",
        "        audio_gap = reduce(audio_layers, 'l b n d -> l b d', 'mean')\n",
        "        audio_embeds = self.audio_norm(audio_gap) * self.audio_gamma\n",
        "        audio_latents = einsum('l b d, l d e -> l b e', audio_embeds, self.audio_latent_weight) + self.audio_latent_bias\n",
        "        audio_latents = l2norm(audio_latents)\n",
        "\n",
        "        text_cls_tokens = text_layers[:, :, 0]\n",
        "        text_embeds = self.text_norm(text_cls_tokens) * self.text_gamma\n",
        "        text_latents = einsum('l b d, l d e -> l b e', text_embeds, self.text_latent_weight) + self.text_latent_bias\n",
        "        text_latents = l2norm(text_latents)\n",
        "\n",
        "        cosine_sims = einsum('l i d, l j d -> l i j', audio_latents, text_latents)\n",
        "\n",
        "        return self.contrast(cosine_sims)\n",
        "\n",
        "# main classes\n",
        "\n",
        "class MuLaN(nn.Module):\n",
        "    @beartype\n",
        "    def __init__(\n",
        "        self,\n",
        "        audio_transformer: AudioSpectrogramTransformer,\n",
        "        text_transformer: TextTransformer,\n",
        "        dim_latent = 128,                       # they use 128\n",
        "        decoupled_contrastive_learning = True,  # think this was used, make it optional\n",
        "        hierarchical_contrastive_loss = False,\n",
        "        hierarchical_contrastive_loss_layers = None,\n",
        "        sigmoid_contrastive_loss = False\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.dim_latent = dim_latent\n",
        "\n",
        "        self.audio = audio_transformer\n",
        "        self.text = text_transformer\n",
        "\n",
        "\n",
        "        self.text_to_latents = nn.Linear(self.text.dim, dim_latent)\n",
        "        self.audio_to_latents = nn.Linear(self.audio.dim, dim_latent)\n",
        "\n",
        "        klass = SigmoidContrastiveLearning if sigmoid_contrastive_loss else partial(SoftmaxContrastiveLearning, decoupled_contrastive_learning = decoupled_contrastive_learning)\n",
        "        self.contrast = klass()\n",
        "\n",
        "        self.multi_layer_contrastive_learning = None\n",
        "\n",
        "        if hierarchical_contrastive_loss:\n",
        "            num_layers = default(hierarchical_contrastive_loss_layers, min(audio_transformer.depth, text_transformer.depth) - 1)\n",
        "            assert num_layers > 0\n",
        "\n",
        "            self.register_buffer('text_layers_indices', interspersed_indices(num_layers, text_transformer.depth))\n",
        "            self.register_buffer('audio_layers_indices', interspersed_indices(num_layers, audio_transformer.depth))\n",
        "\n",
        "            self.multi_layer_contrastive_learning = MultiLayerContrastiveLoss(\n",
        "                audio_dim = self.audio.dim,\n",
        "                text_dim = self.text.dim,\n",
        "                dim_latent = dim_latent,\n",
        "                layers = num_layers,\n",
        "                decoupled_contrastive_learning = decoupled_contrastive_learning,\n",
        "                sigmoid_contrastive_loss = sigmoid_contrastive_loss\n",
        "            )\n",
        "\n",
        "    def get_audio_latents(\n",
        "        self,\n",
        "        wavs,\n",
        "        return_all_layers = False\n",
        "    ):\n",
        "        audio_embeds, audio_layers = self.audio(wavs, return_all_layers = True)\n",
        "        audio_latents = self.audio_to_latents(audio_embeds)\n",
        "        out = l2norm(audio_latents)\n",
        "\n",
        "        if not return_all_layers:\n",
        "            return out\n",
        "\n",
        "        return out, audio_layers\n",
        "\n",
        "    @beartype\n",
        "    def get_text_latents(\n",
        "        self,\n",
        "        texts = None,\n",
        "        raw_texts: Optional[List[str]] = None,\n",
        "        return_all_layers = False\n",
        "    ):\n",
        "        text_embeds, text_layers = self.text(texts, raw_texts = raw_texts, return_all_layers = True)\n",
        "        text_latents = self.text_to_latents(text_embeds)\n",
        "        out = l2norm(text_latents)\n",
        "\n",
        "        if not return_all_layers:\n",
        "            return out\n",
        "\n",
        "        return out, text_layers\n",
        "\n",
        "    @beartype\n",
        "    def forward(\n",
        "        self,\n",
        "        wavs,\n",
        "        texts = None,\n",
        "        raw_texts: Optional[List[str]] = None,\n",
        "        return_latents = False,\n",
        "        return_similarities = False,\n",
        "        return_pairwise_similarities = False\n",
        "    ):\n",
        "        batch, device = wavs.shape[0], wavs.device\n",
        "\n",
        "        audio_latents, audio_layers = self.get_audio_latents(wavs, return_all_layers = True)\n",
        "        text_latents, text_layers = self.get_text_latents(texts, raw_texts = raw_texts, return_all_layers = True)\n",
        "\n",
        "        if return_latents:\n",
        "            return audio_latents, text_latents\n",
        "\n",
        "        if return_similarities:\n",
        "            return einsum('i d, i d -> i', audio_latents, text_latents)\n",
        "\n",
        "        cosine_sim = einsum('i d, j d -> i j', audio_latents, text_latents)\n",
        "\n",
        "        assert cosine_sim.shape[0] == cosine_sim.shape[1], 'batch sizes for audio and text are not equal'\n",
        "\n",
        "        if return_pairwise_similarities:\n",
        "            return cosine_sim\n",
        "\n",
        "        cl_loss = self.contrast(cosine_sim)\n",
        "\n",
        "        if not exists(self.multi_layer_contrastive_learning):\n",
        "            return cl_loss\n",
        "\n",
        "        audio_layers = audio_layers[self.audio_layers_indices]\n",
        "        text_layers = text_layers[self.text_layers_indices]\n",
        "\n",
        "        # whether to do cl loss across all layers, from ViCHA paper https://arxiv.org/abs/2208.13628\n",
        "\n",
        "        hierarchical_cl_loss = self.multi_layer_contrastive_learning(\n",
        "            audio_layers = audio_layers,\n",
        "            text_layers = text_layers\n",
        "        )\n",
        "\n",
        "        return cl_loss + hierarchical_cl_loss\n",
        "\n",
        "# music lm\n",
        "\n",
        "class MuLaNEmbedQuantizer(AudioConditionerBase):\n",
        "    @beartype\n",
        "    def __init__(\n",
        "        self,\n",
        "        mulan: MuLaN,\n",
        "        conditioning_dims: Tuple[int, ...],\n",
        "        rq_num_quantizers = 8,\n",
        "        rq_ema_decay = 0.9,\n",
        "        codebook_size = 1024,\n",
        "        namespaces: Tuple[str, ...] = ('semantic', 'coarse', 'fine'),\n",
        "\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.mulan = mulan\n",
        "\n",
        "        assert len(namespaces) > 0\n",
        "        self.namespaces = namespaces\n",
        "        self.conditioning_dims = conditioning_dims\n",
        "\n",
        "        assert len(conditioning_dims) == len(namespaces), 'number of conditioning dimensions must be equal to number of namespaces'\n",
        "\n",
        "        dim = mulan.dim_latent\n",
        "\n",
        "        self.rq = ResidualVQ(\n",
        "            dim = dim,\n",
        "            num_quantizers = rq_num_quantizers,\n",
        "            codebook_size = codebook_size,\n",
        "            decay = rq_ema_decay,\n",
        "            commitment_weight = 0,    # only use EMA to update codebooks\n",
        "            kmeans_init = True,\n",
        "            threshold_ema_dead_code = 2,\n",
        "            quantize_dropout = False  # no quantize dropout\n",
        "        )\n",
        "\n",
        "        self.dim = dim\n",
        "        self.num_codebooks = rq_num_quantizers\n",
        "\n",
        "        self.cond_embeddings = nn.ParameterDict({})\n",
        "\n",
        "        for namespace, conditioning_dim in zip(namespaces, conditioning_dims):\n",
        "            cond_embeddings = nn.Parameter(torch.randn(rq_num_quantizers, codebook_size, conditioning_dim))\n",
        "            nn.init.normal_(cond_embeddings, std = 0.02)\n",
        "\n",
        "            self.cond_embeddings[namespace] = cond_embeddings\n",
        "\n",
        "        self.set_default_namespace(namespaces[0])\n",
        "\n",
        "    def parameters(self):\n",
        "        return self.cond_embeddings.parameters()\n",
        "\n",
        "    def set_default_namespace(self, namespace):\n",
        "        self._default_namespace = namespace\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        wavs = None,\n",
        "        texts = None,\n",
        "        namespace = None\n",
        "    ):\n",
        "        assert exists(wavs) ^ exists(texts)\n",
        "\n",
        "        namespace = default(namespace, self._default_namespace)\n",
        "        assert namespace in self.namespaces, f'namespace {namespace} not found'\n",
        "        cond_embeddings = self.cond_embeddings[namespace]\n",
        "\n",
        "        with torch.no_grad():\n",
        "            self.mulan.eval()\n",
        "\n",
        "            # sound and language live in joint embedding space because of contrastive learning\n",
        "\n",
        "            if exists(wavs):\n",
        "                latents = self.mulan.get_audio_latents(wavs)\n",
        "            elif exists(texts):\n",
        "                latents = self.mulan.get_text_latents(texts)\n",
        "\n",
        "        _, indices, _ = self.rq(latents)\n",
        "\n",
        "        batch, num_codebooks, dim = indices.shape[0], self.num_codebooks, cond_embeddings.shape[-1]\n",
        "\n",
        "        cond_embeddings = repeat(cond_embeddings, 'q c d -> b q c d', b = batch)\n",
        "        indices = repeat(indices, 'b q -> b q 1 d', q = num_codebooks, d = dim)\n",
        "\n",
        "        cond_embeddings = cond_embeddings.gather(2, indices)\n",
        "        return rearrange(cond_embeddings, 'b q 1 d -> b q d')\n",
        "\n",
        "class MusicLM(nn.Module):\n",
        "    @beartype\n",
        "    def __init__(\n",
        "        self,\n",
        "        audio_lm: AudioLM,\n",
        "        mulan_embed_quantizer: MuLaNEmbedQuantizer\n",
        "    ):\n",
        "        super().__init__()\n",
        "        assert not exists(audio_lm.audio_conditioner), 'mulan must not have been passed into AudioLM. it will be managed externally now, embedding the text into the joint embedding space for text-to-audio synthesis'\n",
        "\n",
        "        self.mulan_embed_quantizer = mulan_embed_quantizer\n",
        "        self.audio_lm = audio_lm\n",
        "\n",
        "    @property\n",
        "    def device(self):\n",
        "        return next(self.parameters()).device\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def forward(\n",
        "        self,\n",
        "        text: str,\n",
        "        num_samples = 1,\n",
        "        **audio_lm_kwargs\n",
        "    ):\n",
        "        self.eval()\n",
        "\n",
        "        texts = tokenizer.tokenize([text]).to(self.device)\n",
        "\n",
        "        text_embeds = self.mulan_embed_quantizer(texts = texts)\n",
        "\n",
        "        # unable to deal with variable lengthed audio for now\n",
        "\n",
        "        samples = []\n",
        "\n",
        "        for _ in range(num_samples):\n",
        "            music = self.audio_lm(text_embeds = text_embeds, **audio_lm_kwargs)\n",
        "            samples.append(music)\n",
        "\n",
        "        # if one sample, just return it\n",
        "\n",
        "        if num_samples == 1:\n",
        "            return first(samples)\n",
        "\n",
        "        mulan = self.mulan_embed_quantizer.mulan\n",
        "\n",
        "        # get the one with the highest similarity score, of all the samples\n",
        "\n",
        "        sims = torch.cat([mulan(texts = texts, wavs = music, return_similarities = True) for music in samples], dim = 0)\n",
        "        top_matching_index = sims.topk(1, dim = 0).indices.item()\n",
        "\n",
        "        return samples[top_matching_index]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "from math import sqrt\n",
        "from random import choice\n",
        "from pathlib import Path\n",
        "from shutil import rmtree\n",
        "from functools import wraps, partial\n",
        "\n",
        "from typing_extensions import Annotated\n",
        "\n",
        "from beartype import beartype\n",
        "from beartype.door import is_bearable\n",
        "from beartype.vale import Is\n",
        "from beartype.typing import Union, List, Optional, Tuple, Callable\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.optim import Adam\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "from lion_pytorch import Lion\n",
        "\n",
        "\n",
        "from einops import rearrange\n",
        "\n",
        "from accelerate import Accelerator\n",
        "\n",
        "# for automatically routing data emitted from a dataset to keywords of the transformer wrappers\n",
        "\n",
        "DATASET_FIELD_TYPE_CONFIG = dict(\n",
        "    wavs = Annotated[\n",
        "        torch.Tensor,\n",
        "        Is[lambda t: t.dtype == torch.float and t.ndim in {2, 3}]\n",
        "    ],\n",
        "    raw_texts = List[str],\n",
        "    texts = Annotated[\n",
        "        torch.Tensor,\n",
        "        Is[lambda t: t.dtype == torch.long and t.ndim == 2]\n",
        "    ],\n",
        ")\n",
        "\n",
        "# helpers\n",
        "\n",
        "def exists(val):\n",
        "    return val is not None\n",
        "\n",
        "def default(*args):\n",
        "    for arg in args:\n",
        "        if exists(arg):\n",
        "            return arg\n",
        "    return None\n",
        "\n",
        "def noop(*args, **kwargs):\n",
        "    pass\n",
        "\n",
        "def cycle(dl):\n",
        "    while True:\n",
        "        for data in dl:\n",
        "            yield data\n",
        "\n",
        "def cast_tuple(t):\n",
        "    return t if isinstance(t, (tuple, list)) else (t,)\n",
        "\n",
        "def yes_or_no(question):\n",
        "    answer = input(f'{question} (y/n) ')\n",
        "    return answer.lower() in ('yes', 'y')\n",
        "\n",
        "def accum_log(log, new_logs):\n",
        "    for key, new_value in new_logs.items():\n",
        "        old_value = log.get(key, 0.)\n",
        "        log[key] = old_value + new_value\n",
        "    return log\n",
        "\n",
        "# auto data to module keyword argument routing functions\n",
        "\n",
        "def has_duplicates(tup):\n",
        "    counts = dict()\n",
        "    for el in tup:\n",
        "        if el not in counts:\n",
        "            counts[el] = 0\n",
        "        counts[el] += 1\n",
        "    return any(filter(lambda count: count > 1, counts.values()))\n",
        "\n",
        "def determine_types(data, config):\n",
        "    output = []\n",
        "    for el in data:\n",
        "        for name, data_type in config.items():\n",
        "            if is_bearable(el, data_type):\n",
        "                output.append(name)\n",
        "                break\n",
        "        else:\n",
        "            raise TypeError(f'unable to determine type of {data}')\n",
        "\n",
        "    return tuple(output)\n",
        "\n",
        "# optimizer functions\n",
        "\n",
        "def separate_weight_decayable_params(params):\n",
        "    wd_params, no_wd_params = [], []\n",
        "    for param in params:\n",
        "        param_list = no_wd_params if param.ndim < 2 else wd_params\n",
        "        param_list.append(param)\n",
        "    return wd_params, no_wd_params\n",
        "\n",
        "# dataloader functions\n",
        "\n",
        "def collate_one_or_multiple_tensors(fn):\n",
        "    @wraps(fn)\n",
        "    def inner(data):\n",
        "        is_one_data = not isinstance(data[0], tuple)\n",
        "\n",
        "        if is_one_data:\n",
        "            data = torch.stack(data)\n",
        "            return (data,)\n",
        "\n",
        "        outputs = []\n",
        "        for datum in zip(*data):\n",
        "            if is_bearable(datum, Tuple[str, ...]):\n",
        "                output = list(datum)\n",
        "            else:\n",
        "                output = fn(datum)\n",
        "\n",
        "            outputs.append(output)\n",
        "\n",
        "        return tuple(outputs)\n",
        "\n",
        "    return inner\n",
        "\n",
        "@collate_one_or_multiple_tensors\n",
        "def curtail_to_shortest_collate(data):\n",
        "    min_len = min(*[datum.shape[0] for datum in data])\n",
        "    data = [datum[:min_len] for datum in data]\n",
        "    return torch.stack(data)\n",
        "\n",
        "@collate_one_or_multiple_tensors\n",
        "def pad_to_longest_fn(data):\n",
        "    return pad_sequence(data, batch_first = True)\n",
        "\n",
        "def get_dataloader(ds, pad_to_longest = True, **kwargs):\n",
        "    collate_fn = pad_to_longest_fn if pad_to_longest else curtail_to_shortest_collate\n",
        "    return DataLoader(ds, collate_fn = collate_fn, **kwargs)\n",
        "\n",
        "# semantic transformer trainer\n",
        "\n",
        "@beartype\n",
        "class MuLaNTrainer(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        mulan: MuLaN,\n",
        "        dataset: Dataset,\n",
        "        *,\n",
        "        num_train_steps = None,\n",
        "        batch_size,\n",
        "        data_max_length = None,\n",
        "        folder = None,\n",
        "        lr = 3e-4,\n",
        "        grad_accum_every = 1,\n",
        "        betas = (0.9, 0.99),\n",
        "        max_grad_norm = 0.5,\n",
        "        valid_frac = 0.05,\n",
        "        random_split_seed = 42,\n",
        "        save_model_every = 1000,\n",
        "        results_folder = './results',\n",
        "        accelerate_kwargs: dict = dict(),\n",
        "        use_lion = False,\n",
        "        force_clear_prev_results = None  # set to True | False to skip the prompt\n",
        "    ):\n",
        "        super().__init__()\n",
        "        assert batch_size > 1, 'batch size must be greater than 1 for contrastive learning (but ideally as large as possible)'\n",
        "\n",
        "        self.accelerator = Accelerator(**accelerate_kwargs)\n",
        "\n",
        "        self.mulan = mulan\n",
        "\n",
        "        self.register_buffer('steps', torch.Tensor([0]))\n",
        "\n",
        "        self.num_train_steps = default(num_train_steps, len(dataset)) # 1 epoch by default\n",
        "        self.batch_size = batch_size\n",
        "        self.grad_accum_every = grad_accum_every\n",
        "\n",
        "        # optimizers\n",
        "\n",
        "        optim_klass = Lion if use_lion else Adam\n",
        "        self.optim = optim_klass(mulan.parameters(), lr = lr, betas = betas)\n",
        "\n",
        "        # max grad norm\n",
        "\n",
        "        self.max_grad_norm = max_grad_norm\n",
        "\n",
        "        self.data_max_length = data_max_length\n",
        "\n",
        "        # create dataset\n",
        "\n",
        "        self.ds = dataset\n",
        "        self.ds_fields = None\n",
        "\n",
        "        # split for validation\n",
        "\n",
        "        if valid_frac > 0:\n",
        "            train_size = int((1 - valid_frac) * len(self.ds))\n",
        "            valid_size = len(self.ds) - train_size\n",
        "            self.ds, self.valid_ds = random_split(self.ds, [train_size, valid_size], generator = torch.Generator().manual_seed(random_split_seed))\n",
        "            self.print(f'training with dataset of {len(self.ds)} samples and validating with randomly splitted {len(self.valid_ds)} samples')\n",
        "        else:\n",
        "            self.valid_ds = self.ds\n",
        "            self.print(f'training with shared training and valid dataset of {len(self.ds)} samples')\n",
        "\n",
        "        # dataloader\n",
        "\n",
        "        self.dl = get_dataloader(self.ds, batch_size = batch_size, shuffle = True, pad_to_longest = False, drop_last = True)\n",
        "\n",
        "        self.valid_dl = get_dataloader(self.valid_ds, batch_size = batch_size, shuffle = True, pad_to_longest = False, drop_last = True)\n",
        "\n",
        "        # prepare with accelerator\n",
        "\n",
        "        (\n",
        "            self.mulan,\n",
        "            self.optim,\n",
        "            self.dl,\n",
        "            self.valid_dl\n",
        "        ) = self.accelerator.prepare(\n",
        "            self.mulan,\n",
        "            self.optim,\n",
        "            self.dl,\n",
        "            self.valid_dl\n",
        "        )\n",
        "\n",
        "        # dataloader iterators\n",
        "\n",
        "        self.dl_iter = cycle(self.dl)\n",
        "        self.valid_dl_iter = cycle(self.valid_dl)\n",
        "\n",
        "        self.save_model_every = save_model_every\n",
        "\n",
        "        hps = dict(\n",
        "            num_train_steps = num_train_steps,\n",
        "            data_max_length = data_max_length,\n",
        "            learning_rate = lr\n",
        "        )\n",
        "\n",
        "        self.accelerator.init_trackers(\"mulan\", config = hps)\n",
        "\n",
        "        # results folder\n",
        "\n",
        "        self.results_folder = Path(results_folder)\n",
        "\n",
        "        if force_clear_prev_results is True or (not exists(force_clear_prev_results) and len([*self.results_folder.glob('**/*')]) > 0 and yes_or_no('do you want to clear previous experiment checkpoints and results?')):\n",
        "            rmtree(str(self.results_folder))\n",
        "\n",
        "        self.results_folder.mkdir(parents = True, exist_ok = True)\n",
        "\n",
        "        # to device\n",
        "\n",
        "        self.mulan.to(self.device)\n",
        "\n",
        "    def save(self, path):\n",
        "        pkg = dict(\n",
        "            model = self.accelerator.get_state_dict(self.mulan),\n",
        "            optim = self.optim.state_dict()\n",
        "        )\n",
        "        torch.save(pkg, path)\n",
        "\n",
        "    def load(self, path):\n",
        "        path = Path(path)\n",
        "        assert path.exists()\n",
        "        pkg = torch.load(str(path), map_location = 'cpu')\n",
        "\n",
        "        mulan = self.accelerator.unwrap_model(self.mulan)\n",
        "        mulan.load_state_dict(pkg['model'])\n",
        "        self.optim.load_state_dict(pkg['optim'])\n",
        "\n",
        "    def print(self, msg):\n",
        "        self.accelerator.print(msg)\n",
        "\n",
        "    @property\n",
        "    def device(self):\n",
        "        return self.accelerator.device\n",
        "\n",
        "    @property\n",
        "    def is_distributed(self):\n",
        "        return not (self.accelerator.distributed_type == DistributedType.NO and self.accelerator.num_processes == 1)\n",
        "\n",
        "    @property\n",
        "    def is_main(self):\n",
        "        return self.accelerator.is_main_process\n",
        "\n",
        "    @property\n",
        "    def is_local_main(self):\n",
        "        return self.accelerator.is_local_main_process\n",
        "\n",
        "    def data_tuple_to_kwargs(self, data):\n",
        "        if not exists(self.ds_fields):\n",
        "            self.ds_fields = determine_types(data, DATASET_FIELD_TYPE_CONFIG)\n",
        "            assert not has_duplicates(self.ds_fields), 'dataset fields must not have duplicate field names'\n",
        "\n",
        "        data_kwargs =  dict(zip(self.ds_fields, data))\n",
        "\n",
        "        wavs = data_kwargs['wavs']\n",
        "        data_kwargs.update(wavs = wavs[..., :self.data_max_length])\n",
        "\n",
        "        return data_kwargs\n",
        "\n",
        "    def train_step(self):\n",
        "        device = self.device\n",
        "\n",
        "        steps = int(self.steps.item())\n",
        "\n",
        "        self.mulan.train()\n",
        "\n",
        "        # logs\n",
        "\n",
        "        logs = {}\n",
        "\n",
        "        # update vae (generator)\n",
        "\n",
        "        for _ in range(self.grad_accum_every):\n",
        "            data_kwargs = self.data_tuple_to_kwargs(next(self.dl_iter))\n",
        "\n",
        "            loss = self.mulan(**data_kwargs)\n",
        "\n",
        "            self.accelerator.backward(loss / self.grad_accum_every)\n",
        "\n",
        "            accum_log(logs, {'loss': loss.item() / self.grad_accum_every})\n",
        "\n",
        "        if exists(self.max_grad_norm):\n",
        "            self.accelerator.clip_grad_norm_(self.mulan.parameters(), self.max_grad_norm)\n",
        "\n",
        "        self.optim.step()\n",
        "        self.optim.zero_grad()\n",
        "\n",
        "        # log\n",
        "\n",
        "        self.print(f\"{steps}: loss: {logs['loss']}\")\n",
        "        self.accelerator.log({\"train_loss\": logs['loss']}, step = steps)\n",
        "\n",
        "        # save model every so often\n",
        "\n",
        "        if self.is_main and not (steps % self.save_model_every):\n",
        "            model_path = str(self.results_folder / f'mulan.{steps}.pt')\n",
        "            self.save(model_path)\n",
        "\n",
        "            self.print(f'{steps}: saving model to {str(self.results_folder)}')\n",
        "\n",
        "        self.steps += 1\n",
        "        return logs\n",
        "\n",
        "    def train(self, log_fn: Callable = noop):\n",
        "\n",
        "        while self.steps < self.num_train_steps:\n",
        "            logs = self.train_step()\n",
        "            log_fn(logs)\n",
        "\n",
        "        self.print('training complete')"
      ],
      "metadata": {
        "id": "AxUBjYPFZygr"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "#from musiclm_pytorch import MuLaN, AudioSpectrogramTransformer, TextTransformer\n",
        "\n",
        "audio_transformer = AudioSpectrogramTransformer(\n",
        "    dim = 512,\n",
        "    depth = 6,\n",
        "    heads = 8,\n",
        "    dim_head = 64,\n",
        "    spec_n_fft = 128,\n",
        "    spec_win_length = 24,\n",
        "    spec_aug_stretch_factor = 0.8\n",
        ")\n",
        "\n",
        "text_transformer = TextTransformer(\n",
        "    dim = 512,\n",
        "    depth = 6,\n",
        "    heads = 8,\n",
        "    dim_head = 64\n",
        ")\n",
        "\n",
        "mulan = MuLaN(\n",
        "    audio_transformer = audio_transformer,\n",
        "    text_transformer = text_transformer\n",
        ")\n",
        "\n",
        "# get a ton of <sound, text> pairs and train\n",
        "\n",
        "wavs = torch.randn(2, 1024)\n",
        "texts = torch.randint(0, 20000, (2, 256))\n",
        "\n",
        "loss = mulan(wavs, texts)\n",
        "loss.backward()\n",
        "\n",
        "# after much training, you can embed sounds and text into a joint embedding space\n",
        "# for conditioning the audio LM\n",
        "\n",
        "embeds = mulan.get_audio_latents(wavs)  # during training\n",
        "\n",
        "embeds = mulan.get_text_latents(texts)  # during inference"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NJzXWb_BaIeq",
        "outputId": "e74a9e21-9fd2-475d-e4b3-4f2c8428f71c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "spectrogram yielded shape of (65, 86), but had to be cropped to (64, 80) to be patchified for transformer\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#from musiclm_pytorch import MuLaNEmbedQuantizer\n",
        "\n",
        "# setup the quantizer with the namespaced conditioning embeddings, unique per quantizer as well as namespace (per transformer)\n",
        "\n",
        "quantizer = MuLaNEmbedQuantizer(\n",
        "    mulan = mulan,                          # pass in trained mulan from above\n",
        "    conditioning_dims = (1024, 1024, 1024), # say all three transformers have model dimensions of 1024\n",
        "    namespaces = ('semantic', 'coarse', 'fine')\n",
        ")\n",
        "\n",
        "# now say you want the conditioning embeddings for semantic transformer\n",
        "\n",
        "wavs = torch.randn(2, 1024)\n",
        "conds = quantizer(wavs = wavs, namespace = 'semantic') # (2, 8, 1024) - 8 is number of quantizers"
      ],
      "metadata": {
        "id": "OWhb_2cyag1g"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets>=1.18.3\n",
        "!pip install transformers==4.11.3\n",
        "!pip install librosa\n",
        "!pip install jiwer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "aJkopGJGbcza",
        "outputId": "53d4d24a-20eb-4997-9458-ab05ce7172dd"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers==4.11.3\n",
            "  Downloading transformers-4.11.3-py3-none-any.whl (2.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers==4.11.3) (1.22.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers==4.11.3) (3.11.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers==4.11.3) (23.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers==4.11.3) (4.65.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers==4.11.3) (2.27.1)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m81.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sacremoses\n",
            "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m880.6/880.6 kB\u001b[0m \u001b[31m41.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers==4.11.3) (6.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.0.17 in /usr/local/lib/python3.9/dist-packages (from transformers==4.11.3) (0.14.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers==4.11.3) (2022.10.31)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.0.17->transformers==4.11.3) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.0.17->transformers==4.11.3) (4.5.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers==4.11.3) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers==4.11.3) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers==4.11.3) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers==4.11.3) (2022.12.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.9/dist-packages (from sacremoses->transformers==4.11.3) (1.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.9/dist-packages (from sacremoses->transformers==4.11.3) (8.1.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.9/dist-packages (from sacremoses->transformers==4.11.3) (1.2.0)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895259 sha256=5e4089ebda48ef12cfd146dae3b95de36d6615ca92401365a218cea07ba07037\n",
            "  Stored in directory: /root/.cache/pip/wheels/12/1c/3d/46cf06718d63a32ff798a89594b61e7f345ab6b36d909ce033\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sacremoses, transformers\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.13.3\n",
            "    Uninstalling tokenizers-0.13.3:\n",
            "      Successfully uninstalled tokenizers-0.13.3\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.28.1\n",
            "    Uninstalling transformers-4.28.1:\n",
            "      Successfully uninstalled transformers-4.28.1\n",
            "Successfully installed sacremoses-0.0.53 tokenizers-0.10.3 transformers-4.11.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "tokenizers",
                  "transformers"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.9/dist-packages (0.10.0.post2)\n",
            "Requirement already satisfied: numpy!=1.22.0,!=1.22.1,!=1.22.2,>=1.20.3 in /usr/local/lib/python3.9/dist-packages (from librosa) (1.22.4)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.9/dist-packages (from librosa) (4.4.2)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.9/dist-packages (from librosa) (0.12.1)\n",
            "Requirement already satisfied: pooch<1.7,>=1.0 in /usr/local/lib/python3.9/dist-packages (from librosa) (1.6.0)\n",
            "Requirement already satisfied: typing-extensions>=4.1.1 in /usr/local/lib/python3.9/dist-packages (from librosa) (4.5.0)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.9/dist-packages (from librosa) (3.0.0)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.9/dist-packages (from librosa) (1.2.2)\n",
            "Requirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.9/dist-packages (from librosa) (0.2)\n",
            "Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.9/dist-packages (from librosa) (1.10.1)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.9/dist-packages (from librosa) (1.0.5)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.9/dist-packages (from librosa) (0.56.4)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.9/dist-packages (from librosa) (0.3.5)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.9/dist-packages (from librosa) (1.2.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from numba>=0.51.0->librosa) (67.7.2)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.9/dist-packages (from numba>=0.51.0->librosa) (0.39.1)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.9/dist-packages (from pooch<1.7,>=1.0->librosa) (2.27.1)\n",
            "Requirement already satisfied: appdirs>=1.3.0 in /usr/local/lib/python3.9/dist-packages (from pooch<1.7,>=1.0->librosa) (1.4.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from pooch<1.7,>=1.0->librosa) (23.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=0.20.0->librosa) (3.1.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.9/dist-packages (from soundfile>=0.12.1->librosa) (1.15.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.9/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.21)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->pooch<1.7,>=1.0->librosa) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->pooch<1.7,>=1.0->librosa) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->pooch<1.7,>=1.0->librosa) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->pooch<1.7,>=1.0->librosa) (2022.12.7)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting jiwer\n",
            "  Downloading jiwer-3.0.1-py3-none-any.whl (21 kB)\n",
            "Requirement already satisfied: click<9.0.0,>=8.1.3 in /usr/local/lib/python3.9/dist-packages (from jiwer) (8.1.3)\n",
            "Collecting rapidfuzz==2.13.7\n",
            "  Downloading rapidfuzz-2.13.7-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: rapidfuzz, jiwer\n",
            "Successfully installed jiwer-3.0.1 rapidfuzz-2.13.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from huggingface_hub import notebook_login\n",
        "\n",
        "# notebook_login()"
      ],
      "metadata": {
        "id": "Qyn7wsiwbvbR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!apt install git-lfs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uMGJ6yY_dRvz",
        "outputId": "bf632b4f-c9c5-4f8e-981e-0efe5088384e"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "git-lfs is already the newest version (2.9.2-1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 24 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import pandas as pd\n",
        "# df = pd.read_csv(\"/content/musiccaps-public.csv\")"
      ],
      "metadata": {
        "id": "Li1APw_wrbUB"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from audiolm_pytorch import HubertWithKmeans, SemanticTransformer, SemanticTransformerTrainer\n",
        "\n",
        "wav2vec = HubertWithKmeans(\n",
        "    checkpoint_path = './hubert/hubert_base_ls960.pt',\n",
        "    kmeans_path = './hubert/hubert_base_ls960_L9_km500.bin'\n",
        ")\n",
        "\n",
        "semantic_transformer = SemanticTransformer(\n",
        "    num_semantic_tokens = wav2vec.codebook_size,\n",
        "    dim = 1024,\n",
        "    depth = 6,\n",
        "    audio_text_condition = True      # this must be set to True (same for CoarseTransformer and FineTransformers)\n",
        ").cuda()\n",
        "\n",
        "trainer = SemanticTransformerTrainer(\n",
        "    transformer = semantic_transformer,\n",
        "    wav2vec = wav2vec,\n",
        "    audio_conditioner = quantizer,   # pass in the MulanEmbedQuantizer instance above\n",
        "    folder ='/path/to/audio/files',\n",
        "    batch_size = 1,\n",
        "    data_max_length = 320 * 32,\n",
        "    num_train_steps = 1\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 387
        },
        "id": "x_sCAtvkagt3",
        "outputId": "033c0de4-139d-45f7-abd3-8da74092ae6a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-79f2b8678289>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0maudiolm_pytorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mHubertWithKmeans\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSemanticTransformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSemanticTransformerTrainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m wav2vec = HubertWithKmeans(\n\u001b[1;32m      5\u001b[0m     \u001b[0mcheckpoint_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'./hubert/hubert_base_ls960.pt'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'audiolm_pytorch'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    }
  ]
}